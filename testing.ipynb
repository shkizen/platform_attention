{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4f60d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b3aca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   kappa  markup_mean  markup_sd  n_seeds\n",
      "0    inf     0.265625   0.027063        4\n",
      "1  100.0     0.012500   0.021651        4\n",
      "2   50.0     0.012500   0.021651        4\n",
      "3   25.0     0.062500   0.081968        4\n",
      "4   10.0     0.075000   0.082916        4\n",
      "5    5.0     0.200000   0.000000        4\n",
      "6    1.0     0.114583   0.080012        4\n",
      "7    0.0    -0.023438   0.025911        4\n",
      "Estimated κ* = 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from platform_attention.kode import SimParams, AgentParams, TrainParams\n",
    "\n",
    "# --- Simulation parameters (environment) ---\n",
    "sim = SimParams(\n",
    "    n=2,                   # number of firms\n",
    "    K=1,                   # memory length\n",
    "    price_grid=(1.0, 1.25, 1.5, 1.75, 2.0),  # 5 prices as you wanted\n",
    "    kappa=np.inf,           # initial attention intensity (will be replaced by grid values)\n",
    "    rho=0.0,                # i.i.d. demand shocks\n",
    "    sigma=0.0,             # shock volatility\n",
    "    b=3.0,                  # demand intercept (optional, default already 3.0)\n",
    "    c=1.0,                  # marginal cost\n",
    "    phi=1.0,                # demand sensitivity\n",
    "    observed_shocks=True,   # public shocks\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# --- Agent parameters (Q-learning) ---\n",
    "ap = AgentParams(\n",
    "    n_actions=len(sim.price_grid),\n",
    "    alpha=0.05,   # learning rate\n",
    "    delta=0.995,  # discount factor\n",
    "    tau0=1.0,     # initial temperature\n",
    "    tau_min=0.01, # minimum temperature\n",
    "    gamma=0.99997 # cooling rate\n",
    ")\n",
    "\n",
    "# --- Training control parameters ---\n",
    "tr = TrainParams(\n",
    "    T_max=50000,      # training horizon per run\n",
    "    replications=4,   # number of random seeds\n",
    "    base_seed=42,\n",
    "    n_jobs=-1         # use all cores for parallel processing\n",
    ")\n",
    "\n",
    "# --- Attention intensity grid to scan ---\n",
    "kap_grid = [np.inf, 100, 50, 25, 10, 5, 1, 0]\n",
    "\n",
    "from kode.threshold import estimate_kappa_star\n",
    "\n",
    "df, kstar = estimate_kappa_star(\n",
    "    out_dir=\"results\",\n",
    "    sim_base=sim,\n",
    "    ap=ap,\n",
    "    kap_grid=kap_grid,\n",
    "    tr=tr,\n",
    "    delta=0.995,\n",
    "    mu=0.3,\n",
    "    tag_base=\"n2_K1\"\n",
    ")\n",
    "\n",
    "print(df)\n",
    "print(\"Estimated κ* =\", kstar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a237ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using platform_attention from: /Users/asifkhan/Downloads/platform_attention/__init__.py\n",
      "Single-vector profits: [0.344666 0.383002]\n",
      "Batch profits shape: (3, 2)\n",
      "Batch profits:\n",
      " [[0.344666 0.383002]\n",
      " [0.383002 0.344666]\n",
      " [0.375    0.375   ]]\n"
     ]
    }
   ],
   "source": [
    "import os, sys, numpy as np\n",
    "\n",
    "# If this notebook is inside platform_attention/, add parent to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import platform_attention as pa\n",
    "\n",
    "print(\"Using platform_attention from:\", pa.__file__)\n",
    "\n",
    "# Build env\n",
    "sim = pa.SimParams(\n",
    "    n=2, K=1,\n",
    "    price_grid=(1.0, 1.25, 1.5, 1.75, 2.0),\n",
    "    kappa=0.0,   # no steering\n",
    "    mu=1.5,      # own-price logit slope\n",
    ")\n",
    "env = pa.AttentionEnv(sim, seed=0)\n",
    "\n",
    "# ---- Single-vector profits ----\n",
    "p = np.array([1.4, 1.6])\n",
    "pi_single = env.static_profits(p, theta=0.0)\n",
    "print(\"Single-vector profits:\", np.round(pi_single, 6))\n",
    "\n",
    "# ---- Batch profits (shape: (B, n)) ----\n",
    "P = np.stack([\n",
    "    np.array([1.4, 1.6]),\n",
    "    np.array([1.6, 1.4]),\n",
    "    np.array([1.5, 1.5]),\n",
    "], axis=0)\n",
    "pi_batch = env.static_profits_batch(P, theta=0.0)\n",
    "print(\"Batch profits shape:\", pi_batch.shape)\n",
    "print(\"Batch profits:\\n\", np.round(pi_batch, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training baseline (kappa=0)...\n",
      "Baseline trained seeds: [42, 59, 76, 93, 110, 127, 144, 161]\n",
      ">>> Generating impulse-response at kappa=0...\n",
      "Baseline impulse figure: results/figures/calvano_n2_K2_kappa0/impulse_response.pdf\n",
      ">>> Estimating kappa* on grid: [inf, 100, 50, 25, 10, 5, 1, 0]\n",
      "   kappa  markup_mean  markup_sd  n_seeds\n",
      "0    inf     0.367708   0.036902        8\n",
      "1  100.0     0.139757   0.030752        8\n",
      "2   50.0     0.142361   0.028421        8\n",
      "3   25.0     0.152778   0.018373        8\n",
      "4   10.0     0.161458   0.013780        8\n",
      "5    5.0     0.187500   0.029463        8\n",
      "6    1.0     0.011719   0.040127        8\n",
      "7    0.0    -0.003472   0.009187        8\n",
      "Estimated kappa* = 0.0\n",
      ">>> Plotting kappa-threshold curve...\n",
      "Saved threshold plot: results/figures/calvano_n2_K2/kappa_threshold.pdf\n",
      ">>> Impulse at kappa ~ kappa*: selected κ = 0.0\n",
      "Impulse figure near kappa*: results/figures/calvano_n2_K2_kap0/impulse_response.pdf\n",
      "\n",
      "✅ Done. Artifacts saved under: /Users/asifkhan/Downloads/platform_attention/results\n",
      "   - Baseline impulse: results/figures/calvano_n2_K2_kappa0/impulse_response.pdf\n",
      "   - Threshold plot:   results/figures/calvano_n2_K2/kappa_threshold.pdf\n",
      "   - Impulse @ κ*:    results/figures/calvano_n2_K2_kap0/impulse_response.pdf\n"
     ]
    }
   ],
   "source": [
    "# === Driver: Baseline (kappa=0) -> Threshold -> Plots (n=2, K=2) ===\n",
    "\n",
    "import os, sys, numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from platform_attention.kode.threshold import estimate_kappa_star\n",
    "from platform_attention.kode.eval import load_greedy_agents\n",
    "from platform_attention.kode.figures import fig_impulse_response, plot_kappa_threshold\n",
    "from platform_attention import SimParams, AgentParams, TrainParams, Trainer, AttentionEnv\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "import numpy as np, os\n",
    "\n",
    "def kappa_to_tag(kappa: float) -> str:\n",
    "    \"\"\"Match the κ tag used during training in estimate_kappa_star: kap{inf|int|with p}.\"\"\"\n",
    "    if np.isinf(kappa):\n",
    "        return \"inf\"\n",
    "    kf = float(kappa)\n",
    "    # Use integer form when possible (e.g., 0.0 -> \"0\", 10.0 -> \"10\")\n",
    "    if kf.is_integer():\n",
    "        return str(int(kf))\n",
    "    return str(kf).replace(\".\", \"p\")\n",
    "\n",
    "def find_any_seed_for_tag(out_dir: str, base_tag: str, kappa: float) -> int:\n",
    "    \"\"\"\n",
    "    Look for a checkpoint directory for this κ. Tries both 'kap' and (fallback) 'kappa' variants.\n",
    "    Returns the first seed it finds.\n",
    "    \"\"\"\n",
    "    tag_core = kappa_to_tag(kappa)\n",
    "    candidates = [\n",
    "        os.path.join(out_dir, \"checkpoints\", f\"{base_tag}_kap{tag_core}\"),\n",
    "        os.path.join(out_dir, \"checkpoints\", f\"{base_tag}_kappa{tag_core}\"),  # fallback to baseline naming style\n",
    "    ]\n",
    "    for ckpt_dir in candidates:\n",
    "        if os.path.isdir(ckpt_dir):\n",
    "            for fname in os.listdir(ckpt_dir):\n",
    "                if \"seed\" in fname and fname.endswith(\".json\"):\n",
    "                    try:\n",
    "                        return int(fname.split(\"seed\")[1].split(\".json\")[0])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    raise FileNotFoundError(f\"No checkpoint directory found for any of {candidates}\")\n",
    "\n",
    "import os\n",
    "\n",
    "def find_seed_exact(out_dir: str, full_tag: str) -> int:\n",
    "    \"\"\"\n",
    "    Find any seed for a checkpoint directory named exactly:\n",
    "      {out_dir}/checkpoints/{full_tag}/\n",
    "    Returns the first seed integer found in filenames like Q_agent0_seed{SEED}.json\n",
    "    \"\"\"\n",
    "    ckpt_dir = os.path.join(out_dir, \"checkpoints\", full_tag)\n",
    "    if not os.path.isdir(ckpt_dir):\n",
    "        raise FileNotFoundError(f\"No checkpoint directory for tag: {ckpt_dir}\")\n",
    "\n",
    "    for fname in os.listdir(ckpt_dir):\n",
    "        if fname.endswith(\".json\") and \"seed\" in fname:\n",
    "            try:\n",
    "                return int(fname.split(\"seed\")[1].split(\".json\")[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "    raise FileNotFoundError(f\"No seed files found in: {ckpt_dir}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 0) Global config (n=2, K=2)\n",
    "# --------------------------\n",
    "OUT_DIR = \"results\"  # keep artifacts inside the package\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Calvano-like environment (no steering), K=2, patient agents.\n",
    "sim = SimParams(\n",
    "    n=2, K=2,\n",
    "    price_grid=(1.0, 1.2, 1.4, 1.6, 1.8),\n",
    "    kappa=0.0,          # NO steering baseline\n",
    "    mu=1.5,             # own-price logit slope\n",
    "    rho=0.0, sigma=0.08,\n",
    "    b=3.0, c=1.0, phi=0.8,\n",
    "    seed=42\n",
    ")\n",
    "ap = AgentParams(\n",
    "    n_actions=len(sim.price_grid),\n",
    "    alpha=0.05, delta=0.997, tau0=1.0, tau_min=0.01, gamma=0.99998\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 1) Train baseline at kappa=0 (Calvano replication)\n",
    "# --------------------------\n",
    "tr_base = TrainParams(T_max=300_000, replications=8, base_seed=42, n_jobs=-1)\n",
    "tag_base0 = \"calvano_n2_K2_kappa0\"\n",
    "\n",
    "print(\">>> Training baseline (kappa=0)...\")\n",
    "trainer = Trainer(sim, ap, out_dir=OUT_DIR)\n",
    "seeds_baseline = trainer.train(tr_base, tag_base0)\n",
    "print(\"Baseline trained seeds:\", seeds_baseline)\n",
    "\n",
    "# Impulse-response at baseline (Calvano Fig-4 analog)\n",
    "print(\">>> Generating impulse-response at kappa=0...\")\n",
    "env0, g_agents0, s0 = load_greedy_agents(OUT_DIR, tag_base0, seeds_baseline[0], sim)\n",
    "fig0, series0 = fig_impulse_response(\n",
    "    env0, g_agents0, s0, out_dir=OUT_DIR, tag=tag_base0,\n",
    "    horizon_after=15, reps=24, warmup_greedy=1000,\n",
    "    deviator_index=0, base_seed=777, n_jobs=-1\n",
    ")\n",
    "print(\"Baseline impulse figure:\", fig0)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Sweep kappa and estimate kappa*\n",
    "# --------------------------\n",
    "kap_grid = [np.inf, 100, 50, 25, 10, 5, 1, 0]  # descending handled inside estimator\n",
    "tr_sweep = TrainParams(T_max=150_000, replications=8, base_seed=99, n_jobs=-1)\n",
    "\n",
    "print(\">>> Estimating kappa* on grid:\", kap_grid)\n",
    "df, kstar = estimate_kappa_star(\n",
    "    out_dir=OUT_DIR,\n",
    "    sim_base=sim,\n",
    "    ap=ap,\n",
    "    kap_grid=kap_grid,\n",
    "    tr=tr_sweep,\n",
    "    delta=ap.delta, mu=sim.mu,\n",
    "    tag_base=\"calvano_n2_K2\",\n",
    "    tol_markup=0.05,  # 5% above Bertrand rule\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(df)\n",
    "print(\"Estimated kappa* =\", kstar)\n",
    "\n",
    "# Plot threshold curve\n",
    "print(\">>> Plotting kappa-threshold curve...\")\n",
    "pdf, png = plot_kappa_threshold(df, kstar, out_dir=OUT_DIR, tag=\"calvano_n2_K2\", tol_markup=0.05)\n",
    "print(\"Saved threshold plot:\", pdf)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Impulse at kappa near kappa* (pick the first competitive kappa)\n",
    "# --------------------------\n",
    "# Choose the smallest kappa with mean markup <= tol (robust visual choice)\n",
    "tol = 0.05\n",
    "eligible = df[df[\"markup_mean\"] <= tol]\n",
    "if not eligible.empty:\n",
    "    kappa_for_impulse = float(eligible.sort_values(\"kappa\").iloc[0][\"kappa\"])\n",
    "else:\n",
    "    # fallback: use kstar if finite, else choose a mid-grid competitive kappa (e.g., 25)\n",
    "    kappa_for_impulse = float(kstar) if np.isfinite(kstar) else 25.0\n",
    "\n",
    "print(f\">>> Impulse at kappa ~ kappa*: selected κ = {kappa_for_impulse}\")\n",
    "\n",
    "# build the exact tag produced by estimate_kappa_star:\n",
    "base_tag = \"calvano_n2_K2\"                       # <- the tag_base you used in estimate_kappa_star\n",
    "kap_tag  = kappa_to_tag(kappa_for_impulse)       # e.g., \"0\", \"10\", \"inf\"\n",
    "tag_kappa = f\"{base_tag}_kap{kap_tag}\"           # e.g., \"calvano_n2_K2_kap0\"\n",
    "\n",
    "# find a seed for that exact tag\n",
    "seed_k = find_seed_exact(OUT_DIR, tag_kappa)\n",
    "\n",
    "# load and run impulse\n",
    "sim_k = SimParams(**{**sim.__dict__, \"kappa\": float(kappa_for_impulse)})\n",
    "envK, g_agentsK, sK = load_greedy_agents(OUT_DIR, tag_kappa, seed_k, sim_k)\n",
    "figK, seriesK = fig_impulse_response(\n",
    "    envK, g_agentsK, sK, out_dir=OUT_DIR, tag=tag_kappa,\n",
    "    horizon_after=15, reps=24, warmup_greedy=1000,\n",
    "    deviator_index=0, base_seed=1234, n_jobs=-1\n",
    ")\n",
    "print(\"Impulse figure near kappa*:\", figK)\n",
    "\n",
    "print(\"\\n✅ Done. Artifacts saved under:\", os.path.abspath(OUT_DIR))\n",
    "print(\"   - Baseline impulse:\", fig0)\n",
    "print(\"   - Threshold plot:  \", pdf)\n",
    "print(\"   - Impulse @ κ*:   \", figK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303c1b8",
   "metadata": {},
   "source": [
    "#### Impulse Response Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ab717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from platform_attention import SimParams, AgentParams, TrainParams\n",
    "from platform_attention.kode.train import Trainer\n",
    "from platform_attention.kode.figures import fig_impulse_response\n",
    "from platform_attention.kode.eval import load_greedy_agents, static_nash_price, static_monopoly_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca363493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained seeds: [42, 59, 76, 93]\n",
      "Checkpoints saved under: results/checkpoints/calvano_n2_K2_kappa0\n"
     ]
    }
   ],
   "source": [
    "# Calvano style i.e. kappa = 0\n",
    "import numpy as np\n",
    "\n",
    "sim = SimParams(\n",
    "    n=3, K=0,\n",
    "    price_grid=(1.0, 1.2, 1.4, 1.6, 1.8),\n",
    "    kappa=0.0,     # no steering\n",
    "    mu=1.5,        # ignored when κ=0 (uniform shares)\n",
    "    b=3.0, c=1.0, phi=0.8,\n",
    "    rho=0.0, sigma=0.08,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ap = AgentParams(\n",
    "    n_actions=len(sim.price_grid),\n",
    "    alpha=0.05, delta=0.997,\n",
    "    tau0=1.0, tau_min=0.01, gamma=0.99998\n",
    ")\n",
    "\n",
    "tr = TrainParams(T_max=T_MAX, replications=REPS_TRAIN, base_seed=BASE_SEED, n_jobs=-1)\n",
    "\n",
    "OUT_DIR = \"results\"                # because the notebook is inside platform_attention/\n",
    "TAG     = \"calvano_n2_K2_kappa0\"   # choose your tag\n",
    "\n",
    "trainer = Trainer(sim, ap, out_dir=OUT_DIR)\n",
    "seeds = trainer.train(tr, TAG)\n",
    "print(\"Trained seeds:\", seeds)\n",
    "print(\"Checkpoints saved under:\", f\"{OUT_DIR}/checkpoints/{TAG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52d9df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impulse figure saved to: results/figures/calvano_n2_K2_kappa0/impulse_response.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load trained run and generate impulse figures\n",
    "# Pick a seed that exists in your checkpoints folder\n",
    "seed = 42\n",
    "env, greedy_agents, start_state = load_greedy_agents(\n",
    "    out_dir=\"results\",\n",
    "    tag=\"calvano_n2_K2_kappa0\",\n",
    "    seed=seed,\n",
    "    sim=sim\n",
    ")\n",
    "\n",
    "# Run impulse-response experiment\n",
    "fig_path, series = fig_impulse_response(\n",
    "    env, greedy_agents, start_state,\n",
    "    out_dir=\"results\",\n",
    "    tag=\"calvano_n2_K2_kappa0\",\n",
    "    horizon_after=15,    # how many periods to plot after the impulse\n",
    "    reps=24,             # averaging replications\n",
    "    warmup_greedy=1000,  # steps to reach the collusive state\n",
    "    deviator_index=0,    # which agent deviates\n",
    "    base_seed=777,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Impulse figure saved to:\", fig_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "499fccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Impulse responses for (n,K) in {(2,1),(2,2),(3,1),(3,2)} — save JSON + PNG ===\n",
    "import os, sys, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ensure we can import the package from the parent\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from platform_attention import SimParams, AgentParams, TrainParams\n",
    "from platform_attention.kode.train import Trainer\n",
    "from platform_attention.kode.eval import load_greedy_agents, static_nash_price, static_monopoly_price\n",
    "from platform_attention.kode.figures import impulse_response_avg_once\n",
    "\n",
    "# ----- Configs you might tweak -----\n",
    "OUT_DIR = \"results\"           # notebook is inside platform_attention/\n",
    "IR_DIR  = os.path.join(OUT_DIR, \"irplots\")\n",
    "os.makedirs(IR_DIR, exist_ok=True)\n",
    "\n",
    "# === Calibrated params: feasible/collusive prices in ~[1.0, 2.0] ===\n",
    "PRICE_GRID = (1.0, 1.2, 1.4, 1.6, 1.8)   # include 2.0 so peak can sit near 1.9–2.0\n",
    "DELTA      = 0.997\n",
    "ALPHA      = 0.05\n",
    "TAU0, TAU_MIN, GAMMA = 1.0, 0.01, 0.99998\n",
    "SIGMA, RHO = 0.08, 0.0\n",
    "\n",
    "# demand / attention primitives\n",
    "A_INTERCEPT = 2.8   # a ≡ b in κ=0 Calvano branch (total Q = a + θ − min p)\n",
    "B_INTERCEPT = 2.8\n",
    "C_COST      = 1.0\n",
    "PHI         = 1.0   # only matters when κ>0\n",
    "MU          = 1.5\n",
    "\n",
    "# training control\n",
    "T_MAX      = 300_000\n",
    "REPS_TRAIN = 4\n",
    "BASE_SEED  = 42\n",
    "\n",
    "# impulse settings\n",
    "HORIZON_AFTER  = 15\n",
    "WARMUP_GREEDY  = 1000\n",
    "DEVIATOR_INDEX = 0\n",
    "REPS_IMPULSE   = 24\n",
    "BASE_SEED_IMP  = 777\n",
    "N_JOBS         = -1\n",
    "\n",
    "def ensure_trained_and_run_ir(n: int, K: int):\n",
    "    \"\"\"\n",
    "    Train (if needed) a baseline κ=0 model for given (n,K),\n",
    "    run impulse response, save JSON and PNG with required filenames.\n",
    "    \"\"\"\n",
    "    tag = f\"calvano_n{n}_K{K}_kappa0\"  # consistent with your prior naming\n",
    "\n",
    "    # --- Build params ---\n",
    "    # baseline for Calvano\n",
    "    sim = SimParams(\n",
    "        n=n, K=K,\n",
    "        price_grid=(1.0, 1.2, 1.4, 1.6, 1.8),\n",
    "        kappa=np.inf,          # <<<<<<<<<<  WTA baseline (Calvano)\n",
    "        a=2.8, b=2.8, c=1.0,\n",
    "        phi=1.0, mu=1.5,       # mu/phi irrelevant in κ=∞ branch\n",
    "        sigma=0.08, rho=0.0,\n",
    "        observed_shocks=True,\n",
    "        seed=BASE_SEED\n",
    "    )\n",
    "\n",
    "    ap = AgentParams(\n",
    "        n_actions=len(sim.price_grid),\n",
    "        alpha=ALPHA, delta=DELTA,\n",
    "        tau0=TAU0, tau_min=TAU_MIN, gamma=GAMMA\n",
    "    )\n",
    "    tr = TrainParams(T_max=T_MAX, replications=REPS_TRAIN, base_seed=BASE_SEED, n_jobs=N_JOBS)\n",
    "\n",
    "    # --- Always train fresh (overwrite checkpoints) ---\n",
    "    ckpt_dir = os.path.join(OUT_DIR, \"checkpoints\", tag)\n",
    "    if os.path.isdir(ckpt_dir):\n",
    "        # remove any existing checkpoints to avoid confusion\n",
    "        import shutil\n",
    "        print(f\"[train] Removing existing checkpoint directory: {ckpt_dir}\")\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    print(f\"[train] Training {tag} (fresh overwrite)...\")\n",
    "\n",
    "    trainer = Trainer(sim, ap, out_dir=OUT_DIR)\n",
    "    seeds = trainer.train(tr, tag)\n",
    "\n",
    "    print(f\"[train] Done. New checkpoints written to {ckpt_dir}\")\n",
    "    print(f\"[train] Seeds: {seeds}\")\n",
    "\n",
    "\n",
    "    # --- Load greedy agents from one seed (pick the first available) ---\n",
    "    seed_pick = None\n",
    "    for f in sorted(os.listdir(ckpt_dir)):\n",
    "        if f.endswith(\".json\") and \"seed\" in f:\n",
    "            try:\n",
    "                seed_pick = int(f.split(\"seed\")[1].split(\".json\")[0])\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if seed_pick is None:\n",
    "        raise FileNotFoundError(f\"No saved Q files found in {ckpt_dir}\")\n",
    "\n",
    "    env, g_agents, s0 = load_greedy_agents(OUT_DIR, tag, seed_pick, sim)\n",
    "\n",
    "    # --- Run impulse-response (averaged) ---\n",
    "    json_path = os.path.join(IR_DIR, f\"ir_n{n}_K{K}.json\")\n",
    "    p_dev, p_riv, theta_bar = impulse_response_avg_once(\n",
    "        env, g_agents, s0,\n",
    "        horizon_after=HORIZON_AFTER,\n",
    "        reps=REPS_IMPULSE,\n",
    "        warmup_greedy=WARMUP_GREEDY,\n",
    "        deviator_index=DEVIATOR_INDEX,\n",
    "        base_seed=BASE_SEED_IMP,\n",
    "        n_jobs=N_JOBS,\n",
    "        out_path=json_path\n",
    "    )\n",
    "    print(f\"[save] JSON -> {json_path}\")\n",
    "\n",
    "    # --- Compute benchmarks at θ̄ ---\n",
    "    pB  = static_nash_price(env, theta_bar)\n",
    "    pM  = static_monopoly_price(env, theta_bar)\n",
    "    pLR = float((p_dev[-1] + p_riv[-1]) / 2.0)\n",
    "\n",
    "    # --- Plot and save PNG (Calvano-style figure) ---\n",
    "    t = np.arange(len(p_dev))\n",
    "    plt.figure(figsize=(6.6, 3.3), dpi=150)\n",
    "    plt.plot(t, p_dev, marker='o', linewidth=2, label=\"Deviating agent\")\n",
    "    plt.plot(t, p_riv, marker='^', linewidth=2, label=\"Nondeviating agent\")\n",
    "    plt.hlines(pB, xmin=t[0], xmax=t[-1], linestyles=\"dotted\",  label=\"Nash price\")\n",
    "    plt.hlines(pM, xmin=t[0], xmax=t[-1], linestyles=\"dashdot\", label=\"Monopoly price\")\n",
    "    plt.hlines(pLR, xmin=t[0], xmax=t[-1], colors=\"gray\", linewidth=1.0, label=\"Long-run price\")\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Price\")\n",
    "    plt.title(f\"Impulse response (n={n}, K={K}, κ=0)\")\n",
    "    plt.legend(frameon=False, ncol=2)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    png_path = os.path.join(IR_DIR, f\"ir_n{n}_K{K}.png\")\n",
    "    plt.savefig(png_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"[save] PNG  -> {png_path}\")\n",
    "\n",
    "    return {\"json\": json_path, \"png\": png_path, \"theta_bar\": theta_bar, \"pB\": pB, \"pM\": pM, \"pLR\": pLR}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run all four combos ----\n",
    "results = {}\n",
    "for n in (2, 3):\n",
    "    for K in (1, 2):\n",
    "        print(f\"\\n=== Running impulse for (n={n}, K={K}) ===\")\n",
    "        results[(n, K)] = ensure_trained_and_run_ir(n, K)\n",
    "\n",
    "print(\"\\nDone. Outputs:\")\n",
    "for (n, K), r in results.items():\n",
    "    print(f\"(n={n},K={K}) -> JSON: {r['json']} | PNG: {r['png']} | θ̄={r['theta_bar']:.3f} | pB={r['pB']:.3f} | pM={r['pM']:.3f} | pLR={r['pLR']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bf9dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (n=2, K=1) ===\n",
      "\n",
      "=== (n=2, K=2) ===\n",
      "No checkpoint found for (n=2, K=2). Training now...\n",
      "Training complete -> tag=n2_K2_auto\n",
      "\n",
      "=== (n=3, K=1) ===\n",
      "No checkpoint found for (n=3, K=1). Training now...\n",
      "Training complete -> tag=n3_K1_auto\n",
      "\n",
      "=== (n=3, K=2) ===\n",
      "No checkpoint found for (n=3, K=2). Training now...\n",
      "Training complete -> tag=n3_K2_auto\n",
      "\n",
      "=== Completed ===\n",
      "- results/irplots/ir_n2_K1.png\n",
      "  results/irplots/ir_n2_K1.json\n",
      "  (tag=n2_K1_kap0, seed=42)\n",
      "- results/irplots/ir_n2_K2.png\n",
      "  results/irplots/ir_n2_K2.json\n",
      "  (tag=n2_K2_auto, seed=42)\n",
      "- results/irplots/ir_n3_K1.png\n",
      "  results/irplots/ir_n3_K1.json\n",
      "  (tag=n3_K1_auto, seed=42)\n",
      "- results/irplots/ir_n3_K2.png\n",
      "  results/irplots/ir_n3_K2.json\n",
      "  (tag=n3_K2_auto, seed=42)\n"
     ]
    }
   ],
   "source": [
    "# IR batch with auto-train fallback using Trainer/TrainParams\n",
    "# Output: results/irplots/ir_n{n}_K{K}.json / .png for (n,K) in {(2,1),(2,2),(3,1),(3,2)}\n",
    "\n",
    "import os, re, json, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure we can import the package when running from .../platform_attention\n",
    "if (os.path.isdir(\"./platform_attention\") and \"./\" not in sys.path):\n",
    "    sys.path.insert(0, \"./\")\n",
    "\n",
    "from platform_attention import (\n",
    "    SimParams,\n",
    "    Trainer, TrainParams,\n",
    "    AgentParams,           # <-- we'll construct this for Trainer\n",
    ")\n",
    "from platform_attention.kode.eval import (\n",
    "    impulse_response_avg_once,\n",
    "    static_nash_price,\n",
    "    static_monopoly_price,\n",
    "    load_greedy_agents,\n",
    ")\n",
    "from platform_attention.kode.io import ensure_dir, load_json\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _first_tag_for_nK(checkpoints_dir: str, n: int, K: int):\n",
    "    \"\"\"Return first tag matching 'n{n}_K{K}_*' or None.\"\"\"\n",
    "    if not os.path.isdir(checkpoints_dir):\n",
    "        return None\n",
    "    prefix = f\"n{n}_K{K}_\"\n",
    "    cands = sorted(d for d in os.listdir(checkpoints_dir) if d.startswith(prefix))\n",
    "    return cands[0] if cands else None\n",
    "\n",
    "_seed_re = re.compile(r\"_seed(\\d+)\\.json$\")\n",
    "def _one_seed_for_tag(tag_dir: str, n_agents: int) -> int:\n",
    "    \"\"\"Pick a seed that has Q files for all agent indices 0..n_agents-1.\"\"\"\n",
    "    files = [f for f in os.listdir(tag_dir) if f.endswith(\".json\")]\n",
    "    seeds = {}\n",
    "    for fn in files:\n",
    "        m = _seed_re.search(fn)\n",
    "        if not m: \n",
    "            continue\n",
    "        sd = int(m.group(1))\n",
    "        m2 = re.search(r\"Q_agent(\\d+)_seed\", fn)\n",
    "        if not m2:\n",
    "            continue\n",
    "        ai = int(m2.group(1))\n",
    "        seeds.setdefault(sd, set()).add(ai)\n",
    "    for sd, have in sorted(seeds.items()):\n",
    "        if have == set(range(n_agents)):\n",
    "            return sd\n",
    "    raise RuntimeError(f\"No seed in {tag_dir} has all {n_agents} agent Q-files.\")\n",
    "\n",
    "def _simparams_from_meta(meta: dict) -> SimParams:\n",
    "    \"\"\"Rebuild SimParams from saved Q meta (only recognized fields are used).\"\"\"\n",
    "    fields = {\"n\",\"c\",\"a\",\"a0\",\"mu\",\"price_grid\",\"K\",\"seed\",\"m\",\"xi\",\"b\",\"phi\",\"sigma\",\"rho\",\"observed_shocks\",\"kappa\"}\n",
    "    kwargs = {k: meta[k] for k in meta.keys() if k in fields}\n",
    "    if \"price_grid\" in kwargs and isinstance(kwargs[\"price_grid\"], list):\n",
    "        kwargs[\"price_grid\"] = tuple(kwargs[\"price_grid\"])\n",
    "    return SimParams(**kwargs)\n",
    "\n",
    "# ---------- config ----------\n",
    "pairs = [(2,1), (2,2), (3,1), (3,2)]\n",
    "root_results = \"results\"\n",
    "ckpt_root = os.path.join(root_results, \"checkpoints\")\n",
    "out_dir = ensure_dir(os.path.join(root_results, \"irplots\"))\n",
    "\n",
    "HORIZON_AFTER = 15\n",
    "REPS = 24\n",
    "WARMUP = 1000\n",
    "DEVIATOR_INDEX = 0\n",
    "BASE_SEED = 777\n",
    "N_JOBS = -1\n",
    "\n",
    "# Train defaults (adjust if you want)\n",
    "train_cfg = TrainParams(\n",
    "    T_max=120_000,\n",
    "    replications=16,\n",
    "    base_seed=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "made = []\n",
    "for (n, K) in pairs:\n",
    "    print(f\"\\n=== (n={n}, K={K}) ===\")\n",
    "    tag = _first_tag_for_nK(ckpt_root, n, K)\n",
    "\n",
    "    if tag is None:\n",
    "        # --------- auto-train fallback ----------\n",
    "        print(f\"No checkpoint found for (n={n}, K={K}). Training now...\")\n",
    "        sim_for_train = SimParams(n=n, K=K)  # uses your env defaults for the rest\n",
    "\n",
    "        # Build AgentParams for Trainer. Trainer internally sets n_actions from env,\n",
    "        # so we only need placeholders for the learning hyperparams; defaults are fine.\n",
    "        # AgentParams requires n_actions; use the grid length as a safe placeholder.\n",
    "        n_actions_placeholder = len(sim_for_train.price_grid)\n",
    "        ap_for_train = AgentParams(n_actions=n_actions_placeholder)\n",
    "\n",
    "        # Tag must start with n{n}_K{K}_ so downstream lookup works\n",
    "        tag = f\"n{n}_K{K}_auto\"\n",
    "\n",
    "        # Instantiate Trainer with sim + ap and results dir\n",
    "        trainer = Trainer(sim_for_train, ap_for_train, out_dir=\"results\")\n",
    "\n",
    "        # Run training (saves Q tables under results/checkpoints/{tag}/)\n",
    "        _ = trainer.train(train_cfg, tag=tag)\n",
    "        print(f\"Training complete -> tag={tag}\")\n",
    "\n",
    "    tag_dir = os.path.join(ckpt_root, tag)\n",
    "    seed = _one_seed_for_tag(tag_dir, n_agents=n)\n",
    "\n",
    "    # read meta to rebuild the exact SimParams that were trained\n",
    "    sample = [f for f in os.listdir(tag_dir) if f.startswith(\"Q_agent0_\") and f\"_seed{seed}.\" in f]\n",
    "    if not sample:\n",
    "        sample = [f for f in os.listdir(tag_dir) if f\"_seed{seed}.\" in f]\n",
    "    if not sample:\n",
    "        raise FileNotFoundError(f\"No Q-table found for (n={n},K={K}) seed={seed}\")\n",
    "    meta = load_json(os.path.join(tag_dir, sample[0])).get(\"meta\", {})\n",
    "    sim = _simparams_from_meta(meta)\n",
    "\n",
    "    # load greedy agents + steady state\n",
    "    env, greedy_agents, start_state = load_greedy_agents(out_dir=root_results, tag=tag, seed=seed, sim=sim)\n",
    "\n",
    "    # impulse response\n",
    "    p_dev, p_riv, theta_bar = impulse_response_avg_once(\n",
    "        env, greedy_agents, start_state,\n",
    "        horizon_after=HORIZON_AFTER, reps=REPS, warmup_greedy=WARMUP,\n",
    "        deviator_index=DEVIATOR_INDEX, base_seed=BASE_SEED, n_jobs=N_JOBS\n",
    "    )\n",
    "\n",
    "    # static benchmarks (only defined for n=2 in your helpers)\n",
    "    has_two_firm_bench = (sim.n == 2)\n",
    "    if has_two_firm_bench:\n",
    "        p_ne  = static_nash_price(env, theta_bar)\n",
    "        p_mon = static_monopoly_price(env, theta_bar)\n",
    "    else:\n",
    "        p_ne, p_mon = np.nan, np.nan\n",
    "    p_lr  = float((p_dev[-1] + p_riv[-1]) / 2.0)\n",
    "\n",
    "    # save JSON\n",
    "    base = os.path.join(out_dir, f\"ir_n{n}_K{K}\")\n",
    "    with open(base + \".json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"meta\": sim.__dict__,\n",
    "            \"tag\": tag,\n",
    "            \"seed\": seed,\n",
    "            \"theta_bar\": float(theta_bar),\n",
    "            \"t\": list(range(len(p_dev))),\n",
    "            \"p_dev\": [float(x) for x in p_dev],\n",
    "            \"p_riv\": [float(x) for x in p_riv],\n",
    "            \"p_ne\":  None if not has_two_firm_bench else float(p_ne),\n",
    "            \"p_mon\": None if not has_two_firm_bench else float(p_mon),\n",
    "            \"p_long_run\": float(p_lr)\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # plot\n",
    "    t = np.arange(len(p_dev))\n",
    "    plt.figure(figsize=(6.8,3.4), dpi=150)\n",
    "    plt.plot(t, p_dev, marker=\"o\", linewidth=2, label=\"Deviator\")\n",
    "    plt.plot(t, p_riv, marker=\"^\", linewidth=2, label=\"Rival\")\n",
    "    if has_two_firm_bench:\n",
    "        plt.hlines(p_ne,  xmin=t[0], xmax=t[-1], linestyles=\"dotted\",  label=\"Nash price\")\n",
    "        plt.hlines(p_mon, xmin=t[0], xmax=t[-1], linestyles=\"dashdot\", label=\"Monopoly price\")\n",
    "    plt.hlines(p_lr,  xmin=t[0], xmax=t[-1], linewidth=1.0, label=\"Long-run price\")\n",
    "    plt.title(f\"Impulse response (n={n}, K={K}) | tag={tag}\")\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Price\")\n",
    "    plt.legend(frameon=False, ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base + \".png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    made.append((base + \".png\", base + \".json\", tag, seed))\n",
    "\n",
    "print(\"\\n=== Completed ===\")\n",
    "for png, jsn, tag, sd in made:\n",
    "    print(f\"- {png}\\n  {jsn}\\n  (tag={tag}, seed={sd})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "492c2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (n=2, K=1) ===\n",
      "\n",
      "=== (n=2, K=2) ===\n",
      "\n",
      "=== (n=3, K=1) ===\n",
      "\n",
      "=== (n=3, K=2) ===\n",
      "\n",
      "=== Completed ===\n",
      "- results/irplots/ir_n2_K1.png\n",
      "  results/irplots/ir_n2_K1.json\n",
      "  (tag=n2_K1_kap0, seed=42)\n",
      "- results/irplots/ir_n2_K2.png\n",
      "  results/irplots/ir_n2_K2.json\n",
      "  (tag=n2_K2_auto, seed=42)\n",
      "- results/irplots/ir_n3_K1.png\n",
      "  results/irplots/ir_n3_K1.json\n",
      "  (tag=n3_K1_auto, seed=42)\n",
      "- results/irplots/ir_n3_K2.png\n",
      "  results/irplots/ir_n3_K2.json\n",
      "  (tag=n3_K2_auto, seed=42)\n"
     ]
    }
   ],
   "source": [
    "# Lagged-IR + smoothing (no repo edits). Produces 4 JSONs/PNGs for (n,K)=(2,1),(2,2),(3,1),(3,2).\n",
    "# Files: results/irplots/ir_n{n}_K{K}.json / .png\n",
    "\n",
    "import os, re, json, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure local import\n",
    "if (os.path.isdir(\"./platform_attention\") and \"./\" not in sys.path):\n",
    "    sys.path.insert(0, \"./\")\n",
    "\n",
    "from platform_attention import SimParams, Trainer, TrainParams, AgentParams\n",
    "from platform_attention.kode.eval import load_greedy_agents, static_nash_price, static_monopoly_price\n",
    "from platform_attention.kode.io import ensure_dir, load_json\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def _first_tag_for_nK(checkpoints_dir: str, n: int, K: int):\n",
    "    if not os.path.isdir(checkpoints_dir):\n",
    "        return None\n",
    "    prefix = f\"n{n}_K{K}_\"\n",
    "    cands = sorted(d for d in os.listdir(checkpoints_dir) if d.startswith(prefix))\n",
    "    return cands[0] if cands else None\n",
    "\n",
    "_seed_re = re.compile(r\"_seed(\\d+)\\.json$\")\n",
    "def _one_seed_for_tag(tag_dir: str, n_agents: int) -> int:\n",
    "    files = [f for f in os.listdir(tag_dir) if f.endswith(\".json\")]\n",
    "    seeds = {}\n",
    "    for fn in files:\n",
    "        m = _seed_re.search(fn)\n",
    "        if not m: \n",
    "            continue\n",
    "        sd = int(m.group(1))\n",
    "        m2 = re.search(r\"Q_agent(\\d+)_seed\", fn)\n",
    "        if not m2:\n",
    "            continue\n",
    "        ai = int(m2.group(1))\n",
    "        seeds.setdefault(sd, set()).add(ai)\n",
    "    for sd, have in sorted(seeds.items()):\n",
    "        if have == set(range(n_agents)):\n",
    "            return sd\n",
    "    raise RuntimeError(f\"No seed in {tag_dir} has all {n_agents} agent Q-files.\")\n",
    "\n",
    "def _simparams_from_meta(meta: dict) -> SimParams:\n",
    "    fields = {\"n\",\"c\",\"a\",\"a0\",\"mu\",\"price_grid\",\"K\",\"seed\",\"m\",\"xi\",\"b\",\"phi\",\"sigma\",\"rho\",\"observed_shocks\",\"kappa\"}\n",
    "    kwargs = {k: meta[k] for k in meta.keys() if k in fields}\n",
    "    if \"price_grid\" in kwargs and isinstance(kwargs[\"price_grid\"], list):\n",
    "        kwargs[\"price_grid\"] = tuple(kwargs[\"price_grid\"])\n",
    "    return SimParams(**kwargs)\n",
    "\n",
    "def _moving_average(x, w: int):\n",
    "    if w is None or w <= 1:\n",
    "        return np.array(x, dtype=float)\n",
    "    w = int(w)\n",
    "    pad = w - 1\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    # simple centered moving average (pad on both ends)\n",
    "    xpad = np.pad(x, (pad//2, pad - pad//2), mode=\"edge\")\n",
    "    ma = np.convolve(xpad, np.ones(w)/w, mode=\"valid\")\n",
    "    return ma\n",
    "\n",
    "# --------- Lagged impulse response runner (per the user's expectation) ----------\n",
    "def impulse_response_avg_once_lagged(env, greedy_agents, start_state, *,\n",
    "                                     horizon_after=55, reps=24, warmup_greedy=1000,\n",
    "                                     deviator_index=0, base_seed=777, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Same as your default IR, except at τ=1 rivals keep their τ=0 prices (one-period lagged response).\n",
    "    From τ≥2, everyone plays greedy again.\n",
    "    \"\"\"\n",
    "    from joblib import Parallel, delayed\n",
    "    from platform_attention.kode.env import AttentionEnv\n",
    "\n",
    "    # Warm to steady state using greedy\n",
    "    s = start_state\n",
    "    def act_g(ag, st): return ag.act_greedy(st)\n",
    "    for _ in range(warmup_greedy):\n",
    "        acts = [act_g(a, s) for a in greedy_agents]\n",
    "        s, _, _ = env.step(acts)\n",
    "\n",
    "    steady_hist = list(env.hist); steady_theta = float(env.theta)\n",
    "    A = env.A\n",
    "    n = env.n\n",
    "\n",
    "    def run_once(rep_seed):\n",
    "        env_r = AttentionEnv(env.prm, seed=rep_seed)\n",
    "        env_r.hist = list(steady_hist); env_r.theta = steady_theta\n",
    "        s0 = (tuple(steady_hist), float(steady_theta)) if env_r.prm.observed_shocks else tuple(steady_hist)\n",
    "\n",
    "        # τ=0: everybody greedy\n",
    "        acts0 = [act_g(a, s0) for a in greedy_agents]\n",
    "        s1, _, info0 = env_r.step(acts0)\n",
    "        p0 = info0[\"p_vec\"].astype(float); th0 = float(info0[\"theta\"])\n",
    "\n",
    "        dev, riv = [p0[deviator_index]], [p0[1 - deviator_index]]\n",
    "\n",
    "        # τ=1 (DEVIATOR undercuts; RIVALS HOLD their τ=0 prices = one-period lag)\n",
    "        # deviator undercuts rival-by-one-tick, capped by deviator's static BR vs rival's τ=0 price\n",
    "        rival0 = p0[1 - deviator_index]\n",
    "        r0_idx = int(np.abs(A - rival0).argmin())\n",
    "        undercut_idx = max(0, r0_idx - 1)\n",
    "        # deviator's static BR to rival0 at current theta\n",
    "        def _static_best_response(envX, rival_price, i, theta):\n",
    "            A_ = envX.A; best_p, best_v = None, -1e300\n",
    "            for p in A_:\n",
    "                pv = np.full(envX.n, rival_price); pv[i] = p\n",
    "                v = envX.static_profits(pv, theta)[i]\n",
    "                if (v > best_v) or (np.isclose(v, best_v) and (best_p is not None) and (p < best_p)):\n",
    "                    best_p, best_v = float(p), v\n",
    "            return best_p\n",
    "        p_dev_br = _static_best_response(env_r, rival0, i=deviator_index, theta=th0)\n",
    "        br_idx = int(np.abs(A - p_dev_br).argmin())\n",
    "        dev_idx = min(undercut_idx, br_idx)\n",
    "\n",
    "        # Build τ=1 actions: rivals keep τ=0 price indices; deviator deviates\n",
    "        acts1 = []\n",
    "        for j in range(n):\n",
    "            if j == deviator_index:\n",
    "                acts1.append(dev_idx)\n",
    "            else:\n",
    "                pj0 = p0[j]\n",
    "                pj0_idx = int(np.abs(A - pj0).argmin())\n",
    "                acts1.append(pj0_idx)\n",
    "\n",
    "        s2, _, info1 = env_r.step(acts1)\n",
    "        p1 = info1[\"p_vec\"].astype(float)\n",
    "        dev.append(p1[deviator_index]); riv.append(p1[1 - deviator_index])\n",
    "\n",
    "        # τ≥2: everyone greedy\n",
    "        s = s2\n",
    "        for _ in range(horizon_after - 1):\n",
    "            acts = [act_g(a, s) for a in greedy_agents]\n",
    "            s, _, info = env_r.step(acts); p = info[\"p_vec\"].astype(float)\n",
    "            dev.append(p[deviator_index]); riv.append(p[1 - deviator_index])\n",
    "\n",
    "        return np.array(dev), np.array(riv), th0\n",
    "\n",
    "    seeds = [base_seed + 13*r for r in range(reps)]\n",
    "    results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(delayed(run_once)(sd) for sd in seeds)\n",
    "    dev_paths, riv_paths, thetas = zip(*results)\n",
    "    dev_mean = np.mean(np.stack(dev_paths), axis=0)\n",
    "    riv_mean = np.mean(np.stack(riv_paths), axis=0)\n",
    "    theta_bar = float(np.mean(thetas))\n",
    "    return dev_mean, riv_mean, theta_bar\n",
    "\n",
    "# ---------------- main batch ----------------\n",
    "pairs = [(2,1), (2,2), (3,1), (3,2)]\n",
    "root_results = \"results\"\n",
    "ckpt_root = os.path.join(root_results, \"checkpoints\")\n",
    "out_dir = ensure_dir(os.path.join(root_results, \"irplots\"))\n",
    "\n",
    "# IR controls\n",
    "HORIZON_AFTER = 15\n",
    "REPS = 64          # ↑ reps to average more\n",
    "WARMUP = 2000      # ↑ warmup to settle cycle\n",
    "DEVIATOR_INDEX = 0\n",
    "BASE_SEED = 777\n",
    "N_JOBS = -1\n",
    "SMOOTH_WIN = 3     # moving-average window (set to 1/None to disable)\n",
    "\n",
    "train_cfg = TrainParams(T_max=120_000, replications=16, base_seed=42, n_jobs=-1)\n",
    "\n",
    "made = []\n",
    "for (n, K) in pairs:\n",
    "    print(f\"\\n=== (n={n}, K={K}) ===\")\n",
    "    tag = _first_tag_for_nK(ckpt_root, n, K)\n",
    "\n",
    "    if tag is None:\n",
    "        print(f\"No checkpoint for (n={n},K={K}). Training...\")\n",
    "        sim_for_train = SimParams(n=n, K=K)\n",
    "        ap_for_train  = AgentParams(n_actions=len(sim_for_train.price_grid))\n",
    "        tag = f\"n{n}_K{K}_auto\"\n",
    "        trainer = Trainer(sim_for_train, ap_for_train, out_dir=\"results\")\n",
    "        _ = trainer.train(train_cfg, tag=tag)\n",
    "        print(f\"Training complete -> tag={tag}\")\n",
    "\n",
    "    tag_dir = os.path.join(ckpt_root, tag)\n",
    "    seed = _one_seed_for_tag(tag_dir, n_agents=n)\n",
    "\n",
    "    # rebuild SimParams from saved meta\n",
    "    sample = [f for f in os.listdir(tag_dir) if f.startswith(\"Q_agent0_\") and f\"_seed{seed}.\" in f] or \\\n",
    "             [f for f in os.listdir(tag_dir) if f\"_seed{seed}.\" in f]\n",
    "    if not sample:\n",
    "        raise FileNotFoundError(f\"No Q-table found for (n={n},K={K}) seed={seed}\")\n",
    "    meta = load_json(os.path.join(tag_dir, sample[0])).get(\"meta\", {})\n",
    "    sim = _simparams_from_meta(meta)\n",
    "\n",
    "    # load greedy agents + start state\n",
    "    env, greedy_agents, start_state = load_greedy_agents(out_dir=root_results, tag=tag, seed=seed, sim=sim)\n",
    "\n",
    "    # run lagged IR (rival reacts from τ=2 onward)\n",
    "    p_dev, p_riv, theta_bar = impulse_response_avg_once_lagged(\n",
    "        env, greedy_agents, start_state,\n",
    "        horizon_after=HORIZON_AFTER, reps=REPS, warmup_greedy=WARMUP,\n",
    "        deviator_index=DEVIATOR_INDEX, base_seed=BASE_SEED, n_jobs=N_JOBS\n",
    "    )\n",
    "\n",
    "    # optional smoothing (after averaging across reps)\n",
    "    p_dev_s = _moving_average(p_dev, SMOOTH_WIN)\n",
    "    p_riv_s = _moving_average(p_riv, SMOOTH_WIN)\n",
    "\n",
    "    # reference lines (only coded for n=2 helpers)\n",
    "    has_two_firm_bench = (sim.n == 2)\n",
    "    if has_two_firm_bench:\n",
    "        p_ne  = float(static_nash_price(env, theta_bar))\n",
    "        p_mon = float(static_monopoly_price(env, theta_bar))\n",
    "    p_lr  = float((p_dev[-1] + p_riv[-1]) / 2.0)\n",
    "\n",
    "    # save JSON (store both raw and smoothed)\n",
    "    base = os.path.join(out_dir, f\"ir_n{n}_K{K}\")\n",
    "    with open(base + \".json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"meta\": sim.__dict__,\n",
    "            \"tag\": tag,\n",
    "            \"seed\": seed,\n",
    "            \"theta_bar\": float(theta_bar),\n",
    "            \"t\": list(range(len(p_dev))),\n",
    "            \"p_dev\": [float(x) for x in p_dev],\n",
    "            \"p_riv\": [float(x) for x in p_riv],\n",
    "            \"p_dev_smooth\": [float(x) for x in p_dev_s],\n",
    "            \"p_riv_smooth\": [float(x) for x in p_riv_s],\n",
    "            \"p_ne\":  None if not has_two_firm_bench else float(p_ne),\n",
    "            \"p_mon\": None if not has_two_firm_bench else float(p_mon),\n",
    "            \"p_long_run\": float(p_lr)\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # plot (smoothed series)\n",
    "    t = np.arange(len(p_dev_s))\n",
    "    plt.figure(figsize=(6.8,3.4), dpi=150)\n",
    "    plt.plot(t, p_dev_s, marker=\"o\", linewidth=2, label=\"Deviator (smooth)\")\n",
    "    plt.plot(t, p_riv_s, marker=\"^\", linewidth=2, label=\"Rival (smooth)\")\n",
    "    if has_two_firm_bench:\n",
    "        plt.hlines(p_ne,  xmin=t[0], xmax=t[-1], linestyles=\"dotted\",  label=\"Nash price\")\n",
    "        plt.hlines(p_mon, xmin=t[0], xmax=t[-1], linestyles=\"dashdot\", label=\"Monopoly price\")\n",
    "    plt.hlines(p_lr,  xmin=t[0], xmax=t[-1], linewidth=1.0, label=\"Long-run price\")\n",
    "    plt.title(f\"Impulse response (lagged rival) (n={n}, K={K}) | tag={tag}\")\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Price\")\n",
    "    plt.legend(frameon=False, ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base + \".png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    made.append((base + \".png\", base + \".json\", tag, seed))\n",
    "\n",
    "print(\"\\n=== Completed ===\")\n",
    "for png, jsn, tag, sd in made:\n",
    "    print(f\"- {png}\\n  {jsn}\\n  (tag={tag}, seed={sd})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "099a6c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Firms</th>\n",
       "      <th>Mean Lerner Index</th>\n",
       "      <th>Mean Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.369175</td>\n",
       "      <td>1.588551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.371402</td>\n",
       "      <td>1.600691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.376328</td>\n",
       "      <td>1.617096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>1.451544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Firms  Mean Lerner Index  Mean Price\n",
       "0                2           0.369175    1.588551\n",
       "1                3           0.371402    1.600691\n",
       "2                4           0.376328    1.617096\n",
       "3                5           0.308349    1.451544"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CELL 1: Long-run markup & mean price by number of firms (Table 2) ---\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from platform_attention import SimParams, AgentParams, TrainParams, Trainer\n",
    "from platform_attention import AttentionEnv\n",
    "from platform_attention.kode.eval import load_greedy_agents\n",
    "from platform_attention.kode.io import ensure_dir\n",
    "\n",
    "# ---------- config ----------\n",
    "out_dir = \"results\"\n",
    "Ns = [2,3,4,5]\n",
    "K = 1\n",
    "kappa = 0.0          # neutral attention for this table\n",
    "delta = 0.95\n",
    "mu = 0.25\n",
    "price_grid = (1.424,1.464,1.505,1.545,1.586,1.626,1.667,1.707,1.747,1.788,1.828,1.869,1.909,1.950,1.990)\n",
    "\n",
    "ap = AgentParams(n_actions=len(price_grid), alpha=0.05, delta=delta, tau0=1.0, tau_min=0.01, gamma=0.99997)\n",
    "tr = TrainParams(T_max=120_000, replications=8, base_seed=42, n_jobs=-1)  # shrink reps if you need quick test\n",
    "\n",
    "def tag_from(n, K, grid, kappa, delta, mu):\n",
    "    # mirror your io.tag_from_params, inlined to keep the cell self-contained\n",
    "    import numpy as _np\n",
    "    kappa_tag = \"inf\" if _np.isinf(kappa) else str(kappa).replace(\".\",\"p\")\n",
    "    grid_tag = \"g\" + \"-\".join(str(x) for x in grid)\n",
    "    return f\"n{n}_K{K}_{grid_tag}_kap{kappa_tag}_del{str(delta).replace('.','p')}_mu{str(mu).replace('.','p')}\"\n",
    "\n",
    "def simulate_lr_metrics(env, greedy_agents, start_state, burn=2000, T=6000):\n",
    "    s = start_state\n",
    "    prices = []\n",
    "    lerners = []\n",
    "    # burn to steady cycle\n",
    "    g = lambda ag, st: ag.act_greedy(st)\n",
    "    for _ in range(burn):\n",
    "        acts = [g(a, s) for a in greedy_agents]\n",
    "        s, _, _ = env.step(acts)\n",
    "    # record T periods\n",
    "    for _ in range(T):\n",
    "        acts = [g(a, s) for a in greedy_agents]\n",
    "        s, _, info = env.step(acts)\n",
    "        p = info[\"p_vec\"].astype(float)\n",
    "        prices.append(p.mean())\n",
    "        lerners.append(((p - env.prm.c) / p).mean())  # Lerner per-firm then average\n",
    "    return float(np.mean(lerners)), float(np.mean(prices))\n",
    "\n",
    "rows = []\n",
    "for n in Ns:\n",
    "    sim = SimParams(n=n, K=K, kappa=kappa, mu=mu, price_grid=price_grid, c=1.0, a=2.0, a0=0.0, seed=42)\n",
    "    tag = tag_from(n, K, price_grid, kappa, delta, mu)\n",
    "\n",
    "    # Train or reuse checkpoints\n",
    "    Trainer(sim, ap, out_dir=out_dir).train(tr, tag)\n",
    "\n",
    "    # Evaluate across seeds used in training\n",
    "    seeds = [tr.base_seed + 17*i for i in range(tr.replications)]\n",
    "    metrics = []\n",
    "    for sd in seeds:\n",
    "        env, greedy_agents, s0 = load_greedy_agents(out_dir, tag, sd, sim)\n",
    "        Lbar, pbar = simulate_lr_metrics(env, greedy_agents, s0, burn=2000, T=6000)\n",
    "        metrics.append((Lbar, pbar))\n",
    "    L_mean = float(np.mean([m[0] for m in metrics]))\n",
    "    p_mean = float(np.mean([m[1] for m in metrics]))\n",
    "    rows.append({\"Number of Firms\": n, \"Mean Lerner Index\": L_mean, \"Mean Price\": p_mean})\n",
    "\n",
    "df_tab2 = pd.DataFrame(rows).sort_values(\"Number of Firms\").reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "ensure_dir(os.path.join(out_dir, \"tables\"))\n",
    "csv_path = os.path.join(out_dir, \"tables\", \"table2_longrun_markup_meanprice.csv\")\n",
    "tex_path = os.path.join(out_dir, \"tables\", \"table2_longrun_markup_meanprice.tex\")\n",
    "df_tab2.to_csv(csv_path, index=False)\n",
    "with open(tex_path, \"w\") as f:\n",
    "    f.write(df_tab2.to_latex(index=False, float_format=\"%.6f\", caption=\"Long-run Markup and Implied Mean Price by Number of Firms\", label=\"tab:longrun_markup\"))\n",
    "\n",
    "df_tab2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e44ca789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Firms</th>\n",
       "      <th>pB</th>\n",
       "      <th>pM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Firms     pB     pM\n",
       "0                2  1.424  1.909\n",
       "1                3  1.424  1.909\n",
       "2                4  1.424  1.909\n",
       "3                5  1.424  1.909"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CELL 2: Static Bertrand (pB) and Joint–Monopoly (pM) by n (Table 3) ---\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "from platform_attention import SimParams, AttentionEnv\n",
    "from platform_attention.kode.eval import static_nash_price, static_monopoly_price\n",
    "from platform_attention.kode.io import ensure_dir\n",
    "\n",
    "out_dir = \"results\"\n",
    "Ns = [2,3,4,5]\n",
    "price_grid = (1.424,1.464,1.505,1.545,1.586,1.626,1.667,1.707,1.747,1.788,1.828,1.869,1.909,1.950,1.990)\n",
    "\n",
    "rows = []\n",
    "for n in Ns:\n",
    "    sim = SimParams(n=n, K=2, kappa=10.0, mu=0.25, price_grid=price_grid, c=1.0, a=2.0, a0=0.0)\n",
    "    env = AttentionEnv(sim, seed=123)\n",
    "    theta_eval = 0.0  # or replace with theta_bar from a learned run\n",
    "    pB = static_nash_price(env, theta_eval)\n",
    "    pM = static_monopoly_price(env, theta_eval)\n",
    "    rows.append({\"Number of Firms\": int(n), \"pB\": float(pB), \"pM\": float(pM)})\n",
    "\n",
    "df_tab3 = pd.DataFrame(rows).sort_values(\"Number of Firms\").reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "ensure_dir(os.path.join(out_dir, \"tables\"))\n",
    "csv_path = os.path.join(out_dir, \"tables\", \"table3_pB_pM_by_n.csv\")\n",
    "tex_path = os.path.join(out_dir, \"tables\", \"table3_pB_pM_by_n.tex\")\n",
    "df_tab3.to_csv(csv_path, index=False)\n",
    "with open(tex_path, \"w\") as f:\n",
    "    f.write(df_tab3.to_latex(index=False, float_format=\"%.3f\", caption=\"Bertrand and Joint–Monopoly Prices by Number of Firms\", label=\"tab:pB_pM_n\"))\n",
    "\n",
    "df_tab3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03f3d4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Memory(K)</th>\n",
       "      <th>pB</th>\n",
       "      <th>pM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Memory(K)     pB     pM\n",
       "0          1  1.464  1.909\n",
       "1          2  1.464  1.909\n",
       "2          3  1.464  1.909\n",
       "3          4  1.464  1.909"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CELL 3: Static Bertrand (pB) and Joint–Monopoly (pM) by K (Table 3(b)) ---\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "from platform_attention import SimParams, AttentionEnv\n",
    "from platform_attention.kode.eval import static_nash_price, static_monopoly_price\n",
    "from platform_attention.kode.io import ensure_dir\n",
    "\n",
    "out_dir = \"results\"\n",
    "Ks = [1,2,3,4]\n",
    "n=2\n",
    "price_grid = (1.424,1.464,1.505,1.545,1.586,1.626,1.667,1.707,1.747,1.788,1.828,1.869,1.909,1.950,1.990)\n",
    "\n",
    "rows = []\n",
    "for K in Ks:\n",
    "    sim = SimParams(n=n, K=K, kappa=0.0, mu=0.25, price_grid=price_grid, c=1.0, a=2.0, a0=0.0)\n",
    "    env = AttentionEnv(sim, seed=123)\n",
    "    theta_eval = 0.0  # or replace with theta_bar from a learned run\n",
    "    pB = static_nash_price(env, theta_eval)\n",
    "    pM = static_monopoly_price(env, theta_eval)\n",
    "    rows.append({\"Memory(K)\": int(K), \"pB\": float(pB), \"pM\": float(pM)})\n",
    "\n",
    "df_tab3 = pd.DataFrame(rows).sort_values(\"Memory(K)\").reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "ensure_dir(os.path.join(out_dir, \"tables\"))\n",
    "csv_path = os.path.join(out_dir, \"tables\", \"table3_pB_pM_by_K.csv\")\n",
    "tex_path = os.path.join(out_dir, \"tables\", \"table3_pB_pM_by_K.tex\")\n",
    "df_tab3.to_csv(csv_path, index=False)\n",
    "with open(tex_path, \"w\") as f:\n",
    "    f.write(df_tab3.to_latex(index=False, float_format=\"%.3f\", caption=\"Bertrand and Joint–Monopoly Prices by K\", label=\"tab:pB_pM_K\"))\n",
    "\n",
    "df_tab3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d1dc4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>mean_var_profit_gain</th>\n",
       "      <th>sd_over_seeds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_shock (σ=0, ρ=0)</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.00262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shock (σ=0.15, ρ=0.85)</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.00262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 scenario  mean_var_profit_gain  sd_over_seeds\n",
       "0     no_shock (σ=0, ρ=0)              0.004124        0.00262\n",
       "1  shock (σ=0.15, ρ=0.85)              0.004124        0.00262"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CELL 3: Profit-gain volatility with & without shocks ---\n",
    "\n",
    "import numpy as np, pandas as pd, os\n",
    "from platform_attention import SimParams, AgentParams, TrainParams, Trainer\n",
    "from platform_attention.kode.eval import load_greedy_agents, static_nash_price\n",
    "from platform_attention.kode.io import ensure_dir\n",
    "\n",
    "out_dir = \"results\"\n",
    "price_grid = (1.424,1.464,1.505,1.545,1.586,1.626,1.667,1.707,1.747,1.788,1.828,1.869,1.909,1.950,1.990)\n",
    "\n",
    "# --- train once at sigma=0 (no shocks) ---\n",
    "sim_train = SimParams(n=2, K=1, kappa=0.0, mu=0.25, price_grid=price_grid, c=1.0, a=2.0, a0=0.0, sigma=0.0, rho=0.0)\n",
    "ap = AgentParams(n_actions=len(price_grid), alpha=0.05, delta=0.95, tau0=1.0, tau_min=0.01, gamma=0.99997)\n",
    "tr = TrainParams(T_max=120_000, replications=4, base_seed=2025, n_jobs=-1)\n",
    "tag = f\"voltest_n{sim_train.n}_K{sim_train.K}_sigma0\"\n",
    "\n",
    "Trainer(sim_train, ap, out_dir=out_dir).train(tr, tag)\n",
    "seeds = [tr.base_seed + 17*i for i in range(tr.replications)]\n",
    "\n",
    "def eval_profit_gain_variance(sim_eval, tag, seeds, T=6000, burn=2000):\n",
    "    # evaluate loaded greedy policies on a given env (possibly shocked)\n",
    "    gains = []\n",
    "    for sd in seeds:\n",
    "        env, greedy_agents, s = load_greedy_agents(out_dir, tag, sd, sim_eval)\n",
    "        # burn to steady state\n",
    "        g = lambda ag, st: ag.act_greedy(st)\n",
    "        for _ in range(burn):\n",
    "            acts = [g(a, s) for a in greedy_agents]\n",
    "            s, _, _ = env.step(acts)\n",
    "        # collect profits & Bertrand baseline (at theta_bar)\n",
    "        prices = []; thetas = []; profits = []\n",
    "        for _ in range(T):\n",
    "            acts = [g(a, s) for a in greedy_agents]\n",
    "            s, _, info = env.step(acts)\n",
    "            p_vec = info[\"p_vec\"].astype(float)\n",
    "            theta = float(info[\"theta\"])\n",
    "            pr = env.static_profits(p_vec, theta=theta)  # per-firm profits at time t\n",
    "            prices.append(p_vec.mean()); thetas.append(theta); profits.append(pr.mean())\n",
    "        theta_bar = float(np.mean(thetas))\n",
    "        pB = static_nash_price(env, theta_bar)  # Bertrand price at mean shock\n",
    "        # Bertrand per-firm profit at pB (symmetric)\n",
    "        piB = env.static_profits(np.full(env.n, pB), theta=theta_bar).mean()\n",
    "        # Profit gain path and its variance\n",
    "        gains_path = (np.array(profits) - piB) / max(piB, 1e-12)\n",
    "        gains.append(np.var(gains_path))\n",
    "    return float(np.mean(gains)), float(np.std(gains))\n",
    "\n",
    "# baseline (no shocks)\n",
    "sim_eval_base = SimParams(**{**sim_train.__dict__})\n",
    "var_base_mean, var_base_sd = eval_profit_gain_variance(sim_eval_base, tag, seeds)\n",
    "\n",
    "# shocked evaluation (same greedy policies, now AR(1) θ path)\n",
    "sim_eval_shock = SimParams(**{**sim_train.__dict__})\n",
    "var_shock_mean, var_shock_sd = eval_profit_gain_variance(sim_eval_shock, tag, seeds)\n",
    "\n",
    "df_vol = pd.DataFrame({\n",
    "    \"scenario\": [\"no_shock (σ=0, ρ=0)\", \"shock (σ=0.15, ρ=0.85)\"],\n",
    "    \"mean_var_profit_gain\": [var_base_mean, var_shock_mean],\n",
    "    \"sd_over_seeds\": [var_base_sd, var_shock_sd],\n",
    "})\n",
    "ensure_dir(os.path.join(out_dir, \"tables\"))\n",
    "csv_path = os.path.join(out_dir, \"tables\", \"profit_gain_volatility.csv\")\n",
    "tex_path = os.path.join(out_dir, \"tables\", \"profit_gain_volatility.tex\")\n",
    "df_vol.to_csv(csv_path, index=False)\n",
    "with open(tex_path, \"w\") as f:\n",
    "    f.write(df_vol.to_latex(index=False, float_format=\"%.5f\", caption=\"Profit-gain variance without and with shocks\", label=\"tab:volatility\"))\n",
    "\n",
    "df_vol\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
